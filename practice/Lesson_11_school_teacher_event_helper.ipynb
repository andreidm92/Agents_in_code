{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreidm92/Agents_in_code/blob/main/practice/Lesson_11_school_teacher_event_helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5165286d",
      "metadata": {
        "id": "5165286d"
      },
      "source": [
        "\n",
        "# 🎓 Day 11 — School Teacher: Event Agenda Helper\n",
        "\n",
        "## 📘 Теория\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 YouTubeTranscriptLoader (LlamaIndex)\n",
        "`YouTubeTranscriptReader` — загрузчик субтитров с YouTube. Позволяет извлекать текст и использовать его в индексах LlamaIndex.\n",
        "\n",
        "```python\n",
        "from llama_index.readers import YouTubeTranscriptReader\n",
        "\n",
        "reader = YouTubeTranscriptReader()\n",
        "documents = reader.load_data([\"https://www.youtube.com/watch?v=jNQXAC9IVRw\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Error Recovery (LangGraph)\n",
        "LangGraph поддерживает автоматическое восстановление после ошибок, что критично при сбоях API.\n",
        "\n",
        "```python\n",
        "def main_node(state):\n",
        "    try:\n",
        "        result = run_risky_llm_call(state)\n",
        "        return {\"result\": result}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"__error__\": True}\n",
        "\n",
        "graph.add_conditional_edges(\"main\", lambda state: \"error\" if \"__error__\" in state else \"success\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Conditional Edges (LangGraph)\n",
        "Позволяют создавать ветвления в графе, в зависимости от состояния агента (например, успешно/ошибка).\n",
        "\n",
        "```python\n",
        "graph.add_node(\"process\", process_fn)\n",
        "graph.add_node(\"handle_error\", error_fn)\n",
        "graph.set_entry_point(\"process\")\n",
        "\n",
        "def edge_decision(state):\n",
        "    return \"handle_error\" if state.get(\"error\") else \"next_step\"\n",
        "\n",
        "graph.add_conditional_edges(\"process\", edge_decision)\n",
        "```\n",
        "\n",
        "📌 Применяется для:  \n",
        "- обработки ошибок  \n",
        "- выбора сценария в зависимости от ввода пользователя  \n",
        "- маршрутизации по статусу данных\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Практика\n",
        "\n",
        "Создайте помощника преподавателя, который:\n",
        "1. Принимает ссылку на YouTube.\n",
        "2. Загружает транскрипт.\n",
        "3. Генерирует повестку занятия и краткое резюме.\n",
        "4. Поддерживает retry при ошибке.\n",
        "5. Использует условные переходы LangGraph для обработки ошибок.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Установка\n",
        "\n",
        "```python\n",
        "!pip install llama-index youtube-transcript-api openai\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📥 Загрузка YouTube-транскрипта\n",
        "\n",
        "```python\n",
        "from llama_index.readers import YouTubeTranscriptReader\n",
        "\n",
        "reader = YouTubeTranscriptReader()\n",
        "documents = reader.load_data([\"<ВСТАВЬТЕ_ССЫЛКУ_НА_YOUTUBE>\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Индексация и генерация повестки\n",
        "\n",
        "```python\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "response = index.as_query_engine().query(\"Сформируй повестку занятия и краткое резюме\")\n",
        "\n",
        "print(response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🚨 Обработка ошибок\n",
        "\n",
        "```python\n",
        "try:\n",
        "    response = index.as_query_engine().query(\"Сформируй повестку занятия и краткое резюме\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(\"Ошибка при генерации:\", e)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Conditional Edges: шаблон LangGraph\n",
        "\n",
        "```python\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "def process_node(state):\n",
        "    # Пример вызова LLM или API\n",
        "    if \"fail\" in state.get(\"mode\", \"\"):\n",
        "        raise ValueError(\"Симулированная ошибка\")\n",
        "    return {\"result\": \"Успешно\"}\n",
        "\n",
        "def error_node(state):\n",
        "    return {\"error_handled\": True}\n",
        "\n",
        "graph = StateGraph()\n",
        "graph.add_node(\"process\", process_node)\n",
        "graph.add_node(\"handle_error\", error_node)\n",
        "graph.set_entry_point(\"process\")\n",
        "\n",
        "graph.add_conditional_edges(\"process\", lambda state: \"handle_error\" if \"error\" in state else \"success\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Установка"
      ],
      "metadata": {
        "id": "HibYvZPW_2Zg"
      },
      "id": "HibYvZPW_2Zg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index youtube-transcript-api openai langgraph\n",
        "!pip install -q llama-index-readers-youtube-transcript\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tHmvL1R_ZNh",
        "outputId": "d0f37bf1-9084-4c8d-c1a7-64c6c6cedd22"
      },
      "id": "-tHmvL1R_ZNh",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Вставь OpenAI API ключ: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Sb9GVABSl6",
        "outputId": "d206feca-0e8f-4b3c-dd73-d6a4fe9f8226"
      },
      "id": "l3Sb9GVABSl6",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Вставь OpenAI API ключ: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📥 Загрузка YouTube-транскрипта"
      ],
      "metadata": {
        "id": "j4PAzKqIAJ5X"
      },
      "id": "j4PAzKqIAJ5X"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "\n",
        "# Initialize the reader\n",
        "reader = YoutubeTranscriptReader()\n",
        "\n",
        "# Load data from the YouTube video\n",
        "documents = reader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=J_0qvRt4LNk\"])\n"
      ],
      "metadata": {
        "id": "EfmLOHus_4jD"
      },
      "id": "EfmLOHus_4jD",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 Индексация и генерация повестки"
      ],
      "metadata": {
        "id": "CdxQI6cXBouu"
      },
      "id": "CdxQI6cXBouu"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "response = index.as_query_engine().query(\"О чем этот документ\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmme32K9BqWS",
        "outputId": "3f694abf-8aea-4aed-c33b-91215c1abf57"
      },
      "id": "wmme32K9BqWS",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документ о том, как использовать Lang chain для создания промптов и обучения моделей языка для различных задач, таких как генерация названий ресторанов на основе их описания и нахождение антонимов для заданных слов с помощью примеров.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚨 Обработка ошибок"
      ],
      "metadata": {
        "id": "LY3C3T9u5wQl"
      },
      "id": "LY3C3T9u5wQl"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response = index.as_query_engine().query(\"Для чего нужен Lang chain\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(\"Ошибка при генерации:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct-xwQ_v51aS",
        "outputId": "39b01868-c2f0-49b1-82ef-0b980eddc2eb"
      },
      "id": "ct-xwQ_v51aS",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lang chain is needed to build fully featured apps that interact with the normal software stack, manage the use of large language models and prompts, integrate with traditional software stack components like APIs, tools, calculators, databases, and data sources, and handle prompt templates effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 Conditional Edges: шаблон LangGraph\n",
        "\n",
        "🧩 Conditional Edges в LangGraph — это механизм условных переходов между узлами в LangGraph (фреймворк построения графов состояний для LLM-агентов).\n",
        "\n",
        "📌 Что такое Conditional Edge?\n",
        "Это правило, которое говорит:\n",
        "\n",
        "\"После выполнения узла X — перейди к Y или Z в зависимости от результата.\"\n",
        "\n",
        "🔧 Пример применения:\n",
        "\n",
        "graph.add_conditional_edges(\"process\", lambda state: \"handle_error\" if \"error\" in state else \"next_step\")\n",
        "\n",
        "Здесь:\n",
        "\n",
        "process — текущий узел (Node),\n",
        "\n",
        "lambda state: ... — функция выбора следующего шага,\n",
        "\n",
        "\"handle_error\" — если возникла ошибка,\n",
        "\n",
        "\"next_step\" — если всё прошло успешно.\n",
        "\n",
        "\n",
        "🎯 Когда и зачем это нужно\n",
        "Сценарий\tКак помогает Conditional Edge\n",
        "✅ Обработка ошибок\tЕсли state содержит error, перейти к узлу с retry или логированием\n",
        "🔄 Переформулировка запроса\tЕсли LLM дал низкокачественный ответ — переписать и запустить снова\n",
        "🔀 Ветвление логики\tЕсли пользователь выбрал \"A\" — идём в узел A, если \"B\" — в узел B\n",
        "🧠 LLM выбирает путь\tНа основе промпта агент решает, какой tool или подграф вызывать\n",
        "\n",
        "💡 Как устроено\n",
        "Каждый узел в LangGraph возвращает state: dict.\n",
        "\n",
        "Функция перехода (lambda) смотрит в это состояние и определяет, какой узел активировать дальше.\n",
        "\n",
        "Это альтернатива if/else в обычном коде — но визуализируемая и устойчиво исполняемая как граф."
      ],
      "metadata": {
        "id": "dgRQTZjD6GG5"
      },
      "id": "dgRQTZjD6GG5"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Optional\n",
        "\n",
        "class State(TypedDict, total=False):\n",
        "    mode: Optional[str]\n",
        "    result: Optional[str]\n",
        "    error: Optional[str]\n",
        "    msg: Optional[str]\n",
        "    completed: Optional[bool]\n",
        "    handled: Optional[bool]\n",
        "\n",
        "def process_node(state):\n",
        "    if \"fail\" in state.get(\"mode\", \"\"):\n",
        "        return {\"error\": \"Ошибка в процессе\"}\n",
        "    return {\"result\": \"Успешно\"}\n",
        "\n",
        "def error_node(state):\n",
        "    return {\"handled\": True, \"msg\": \"Произошла ошибка\"}\n",
        "\n",
        "def success_node(state):\n",
        "    return {\"completed\": True, \"msg\": \"Процесс успешен\"}\n",
        "\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"process\", process_node)\n",
        "graph.add_node(\"handle_error\", error_node)\n",
        "graph.add_node(\"success\", success_node)\n",
        "graph.set_entry_point(\"process\")\n",
        "\n",
        "graph.add_conditional_edges(\"process\", lambda state: \"handle_error\" if \"error\" in state else \"success\")\n",
        "\n",
        "app = graph.compile()\n",
        "print(app.invoke({\"mode\": \"normal\"}))\n",
        "print(app.invoke({\"mode\": \"fail\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPIIGoL40Y6C",
        "outputId": "d0e6d50b-ba6c-4a51-b454-b5537ba58edc"
      },
      "id": "oPIIGoL40Y6C",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mode': 'normal', 'result': 'Успешно', 'msg': 'Процесс успешен', 'completed': True}\n",
            "{'mode': 'fail', 'error': 'Ошибка в процессе', 'msg': 'Произошла ошибка', 'handled': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔧 Error Recovery в LangGraph — подробный разбор\n",
        "Error Recovery — это механизм устойчивого восстановления исполнения графа, если в каком-либо узле (node) возникает ошибка (например, сбой вызова LLM, падение API, TimeoutError).\n",
        "\n",
        "❗ Почему это важно\n",
        "При работе с LLM-агентами и API часто возможны:\n",
        "\n",
        "нестабильные ответы модели,\n",
        "\n",
        "таймауты,\n",
        "\n",
        "отсутствующие поля в JSON,\n",
        "\n",
        "превышение лимитов.\n",
        "\n",
        "Без Error Recovery весь pipeline \"падает\". LangGraph позволяет перехватывать и обрабатывать сбои, чтобы продолжить выполнение.\n",
        "\n",
        "🧩 Как реализуется Error Recovery в LangGraph\n",
        "Используется Conditional Edge + try/except в узле.\n",
        "\n",
        "Шаги:\n",
        "В узле (node) оборачиваем вызов risky-операции в try/except.\n",
        "\n",
        "Если исключение — добавляем в state специальный ключ (\"__error__\" или \"error\").\n",
        "\n",
        "Используем add_conditional_edges, чтобы на этом основании перейти в handle_error.\n",
        "\n"
      ],
      "metadata": {
        "id": "kSybhWhWDMsI"
      },
      "id": "kSybhWhWDMsI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAf_HRsO8a-U"
      },
      "id": "qAf_HRsO8a-U",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}